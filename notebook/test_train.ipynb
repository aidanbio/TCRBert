{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-22T08:00:33.560423977Z",
     "start_time": "2023-11-22T08:00:32.530006981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822122/380327161.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:00:33 [INFO]: Loaded exp_conf: {'title': 'testexp', 'description': 'testexp', 'paper': 'ASAP', 'model_config': '../config/bert-base/', 'outdir': '../output/testexp', 'train': {'pretrained_model': {'type': 'tape', 'location': '../config/bert-base/'}, 'data_parallel': True, 'backup': 'train.bak.{date}.tar.gz', 'rounds': [{'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}, {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}]}, 'eval': {'data_parallel': False, 'batch_size': 128, 'n_workers': 12, 'metrics': ['accuracy', 'f1', 'roc_auc'], 'output_attentions': False, 'tests': [{'data': 'shomuradova', 'result': 'eval.shomuradova.result.json'}, {'data': 'immunecode', 'result': 'eval.immunecode.result.json'}]}}\n",
      "2023-11-22 17:00:33 [INFO]: Loaded exp_conf: {'title': 'testexp', 'description': 'testexp', 'paper': 'ASAP', 'model_config': '../config/bert-base/', 'outdir': '../output/testexp', 'train': {'pretrained_model': {'type': 'tape', 'location': '../config/bert-base/'}, 'data_parallel': True, 'backup': 'train.bak.{date}.tar.gz', 'rounds': [{'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}, {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}]}, 'eval': {'data_parallel': False, 'batch_size': 128, 'n_workers': 12, 'metrics': ['accuracy', 'f1', 'roc_auc'], 'output_attentions': False, 'tests': [{'data': 'shomuradova', 'result': 'eval.shomuradova.result.json'}, {'data': 'immunecode', 'result': 'eval.immunecode.result.json'}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'title': 'testexp',\n 'description': 'testexp',\n 'paper': 'ASAP',\n 'model_config': '../config/bert-base/',\n 'outdir': '../output/testexp',\n 'train': {'pretrained_model': {'type': 'tape',\n   'location': '../config/bert-base/'},\n  'data_parallel': True,\n  'backup': 'train.bak.{date}.tar.gz',\n  'rounds': [{'data': 'dash_vdjdb_mcpas',\n    'test_size': 0.2,\n    'batch_size': 256,\n    'n_epochs': 3,\n    'n_workers': 12,\n    'metrics': ['accuracy'],\n    'optimizer': {'type': 'adamw', 'lr': 0.0001},\n    'train_bert_encoders': [-10, None],\n    'early_stopper': {'monitor': 'accuracy', 'patience': 1},\n    'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk',\n     'monitor': 'accuracy',\n     'save_best_only': True,\n     'period': 1},\n    'result': 'train.{round}.result.json'},\n   {'data': 'iedb_sars2',\n    'test_size': 0.2,\n    'batch_size': 256,\n    'n_epochs': 3,\n    'n_workers': 12,\n    'metrics': ['accuracy'],\n    'optimizer': {'type': 'adamw', 'lr': 0.0001},\n    'train_bert_encoders': [-6, None],\n    'early_stopper': {'monitor': 'accuracy', 'patience': 1},\n    'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk',\n     'monitor': 'accuracy',\n     'save_best_only': True,\n     'period': 1},\n    'result': 'train.{round}.result.json'}]},\n 'eval': {'data_parallel': False,\n  'batch_size': 128,\n  'n_workers': 12,\n  'metrics': ['accuracy', 'f1', 'roc_auc'],\n  'output_attentions': False,\n  'tests': [{'data': 'shomuradova', 'result': 'eval.shomuradova.result.json'},\n   {'data': 'immunecode', 'result': 'eval.immunecode.result.json'}]}}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from enum import auto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display\n",
    "\n",
    "rootdir = '/home/hym/trunk/TCRBert'\n",
    "workdir = '%s/notebook' % rootdir\n",
    "datadir = '%s/data' % rootdir\n",
    "srcdir = '%s/tcrbert' % rootdir\n",
    "outdir = '%s/output' % rootdir\n",
    "\n",
    "os.chdir(workdir)\n",
    "\n",
    "sys.path.append(rootdir)\n",
    "sys.path.append(srcdir)\n",
    "\n",
    "from tcrbert.exp import Experiment\n",
    "from tcrbert.predlistener import PredResultRecoder\n",
    "\n",
    "\n",
    "# Display\n",
    "pd.set_option('display.max.rows', 2000)\n",
    "pd.set_option('display.max.columns', 2000)\n",
    "\n",
    "# Logger\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.config.fileConfig('../config/logging.conf')\n",
    "logger = logging.getLogger('tcrbert')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Target experiment\n",
    "exp_key = 'testexp'\n",
    "Experiment.load_exp_conf('testexp', reload=True)\n",
    "experiment = Experiment.from_key(exp_key)\n",
    "\n",
    "exp_conf = experiment.exp_conf\n",
    "\n",
    "display(exp_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     24
    ],
    "ExecuteTime": {
     "end_time": "2023-11-22T08:08:39.351306935Z",
     "start_time": "2023-11-22T08:00:42.578084260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:00:42 [INFO]: shomuradova dataset was loaded from ../output/shomuradova.data.csv, df.shape: (610, 9)\n",
      "2023-11-22 17:00:42 [INFO]: immunecode dataset was loaded from ../output/immunecode.data.csv, df.shape: (742, 9)\n",
      "2023-11-22 17:00:42 [INFO]: ======================\n",
      "2023-11-22 17:00:42 [INFO]: Begin train at 2023-11-22 17:00:42.619333\n",
      "2023-11-22 17:00:42 [INFO]: Loading the TAPE pretrained model from ../config/bert-base/\n",
      "2023-11-22 17:00:43 [INFO]: Using DataParallel model with 2 GPUs\n",
      "2023-11-22 17:00:43 [INFO]: Start 2 train rounds of testexp at 2023-11-22 17:00:42.619333\n",
      "2023-11-22 17:00:43 [INFO]: train_conf: {'pretrained_model': {'type': 'tape', 'location': '../config/bert-base/'}, 'data_parallel': True, 'backup': 'train.bak.{date}.tar.gz', 'rounds': [{'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}, {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}]}\n",
      "2023-11-22 17:00:43 [INFO]: Start 0 train round using data: dash_vdjdb_mcpas, round_conf: {'data': 'dash_vdjdb_mcpas', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-10, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}\n",
      "2023-11-22 17:00:44 [INFO]: dash_vdjdb_mcpas dataset was loaded from ../output/dash_vdjdb_mcpas.data.csv, df.shape: (25138, 10)\n",
      "2023-11-22 17:00:44 [INFO]: Start to exclude eval data from train data, df.shape: (25138, 10), target_cols: ['index']\n",
      "2023-11-22 17:00:44 [INFO]: shomuradova dataset was loaded from ../output/shomuradova.data.csv, df.shape: (610, 9)\n",
      "2023-11-22 17:00:44 [INFO]: Excluding shomuradova eval data by index from train data\n",
      "2023-11-22 17:00:44 [INFO]: Current train data.shape: (25138, 10)\n",
      "2023-11-22 17:00:44 [INFO]: immunecode dataset was loaded from ../output/immunecode.data.csv, df.shape: (742, 9)\n",
      "2023-11-22 17:00:44 [INFO]: Excluding immunecode eval data by index from train data\n",
      "2023-11-22 17:00:44 [INFO]: Current train data.shape: (25138, 10)\n",
      "2023-11-22 17:00:44 [INFO]: Final train data.shape: (25138, 10)\n",
      "2023-11-22 17:00:44 [INFO]: The bert encoders to be trained: [-10, None]\n",
      "2023-11-22 17:00:45 [INFO]: ======================\n",
      "2023-11-22 17:00:45 [INFO]: Begin training...\n",
      "2023-11-22 17:00:45 [INFO]: use_cuda, device: True, cuda:0\n",
      "2023-11-22 17:00:45 [INFO]: train.n_data: 20110, test.n_data: 5028\n",
      "2023-11-22 17:00:45 [INFO]: optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "2023-11-22 17:00:45 [INFO]: evaluator: <tcrbert.model.BertTCREpitopeModel.PredictionEvaluator object at 0x7ff1ec1880d0>\n",
      "2023-11-22 17:00:45 [INFO]: n_epochs: 3\n",
      "2023-11-22 17:00:45 [INFO]: train.batch_size: 256\n",
      "2023-11-22 17:00:45 [INFO]: test.batch_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in epoch 0/3: 100%|██████████| 79/79 [01:00<00:00,  1.31batch/s]\n",
      "Validating in epoch 0/3: 100%|██████████| 20/20 [00:10<00:00,  1.82batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:01:56 [INFO]: [EvalScoreRecoder]: In epoch 0/3, loss train score: 0.6775637993329688, val score: 0.6418470144271851\n",
      "2023-11-22 17:01:56 [INFO]: [EvalScoreRecoder]: In epoch 0/3, accuracy train score: 0.568565168702086, val score: 0.6159489329268293\n",
      "2023-11-22 17:01:56 [INFO]: [EarlyStopper]: In epoch 0/3, accuracy score: 0.6159489329268293, best accuracy score: -inf;update best score to 0.6159489329268293\n",
      "2023-11-22 17:01:56 [INFO]: [ModelCheckpoint]: Checkpoint at epoch 0: accuracy improved from -inf to 0.6159489329268293, saving model to ../output/testexp/train.0.model_0.chk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in epoch 1/3: 100%|██████████| 79/79 [00:59<00:00,  1.34batch/s]\n",
      "Validating in epoch 1/3: 100%|██████████| 20/20 [00:10<00:00,  1.84batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:03:06 [INFO]: [EvalScoreRecoder]: In epoch 1/3, loss train score: 0.5968728804890113, val score: 0.5593952238559723\n",
      "2023-11-22 17:03:06 [INFO]: [EvalScoreRecoder]: In epoch 1/3, accuracy train score: 0.6559282514708504, val score: 0.680201981707317\n",
      "2023-11-22 17:03:06 [INFO]: [EarlyStopper]: In epoch 1/3, accuracy score: 0.680201981707317, best accuracy score: 0.6159489329268293;update best score to 0.680201981707317\n",
      "2023-11-22 17:03:06 [INFO]: [ModelCheckpoint]: Checkpoint at epoch 1: accuracy improved from 0.6159489329268293 to 0.680201981707317, saving model to ../output/testexp/train.0.model_1.chk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in epoch 2/3: 100%|██████████| 79/79 [00:58<00:00,  1.35batch/s]\n",
      "Validating in epoch 2/3: 100%|██████████| 20/20 [00:10<00:00,  1.84batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:04:16 [INFO]: [EvalScoreRecoder]: In epoch 2/3, loss train score: 0.5154932401602781, val score: 0.5430145025253296\n",
      "2023-11-22 17:04:16 [INFO]: [EvalScoreRecoder]: In epoch 2/3, accuracy train score: 0.7241661002406846, val score: 0.707398056402439\n",
      "2023-11-22 17:04:16 [INFO]: [EarlyStopper]: In epoch 2/3, accuracy score: 0.707398056402439, best accuracy score: 0.680201981707317;update best score to 0.707398056402439\n",
      "2023-11-22 17:04:16 [INFO]: [ModelCheckpoint]: Checkpoint at epoch 2: accuracy improved from 0.680201981707317 to 0.707398056402439, saving model to ../output/testexp/train.0.model_2.chk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:04:16 [INFO]: [EvalScoreRecoder]: loss train socres: [0.6775637993329688, 0.5968728804890113, 0.5154932401602781], val scores: [0.6418470144271851, 0.5593952238559723, 0.5430145025253296]\n",
      "2023-11-22 17:04:16 [INFO]: [EvalScoreRecoder]: accuracy train socres: [0.568565168702086, 0.6559282514708504, 0.7241661002406846], val scores: [0.6159489329268293, 0.680201981707317, 0.707398056402439]\n",
      "2023-11-22 17:04:16 [INFO]: End training...\n",
      "2023-11-22 17:04:16 [INFO]: ======================\n",
      "2023-11-22 17:04:16 [INFO]: 0 train round result: {'metrics': ['accuracy'], 'train.score': OrderedDict([('loss', [0.6775637993329688, 0.5968728804890113, 0.5154932401602781]), ('accuracy', [0.568565168702086, 0.6559282514708504, 0.7241661002406846])]), 'val.score': OrderedDict([('loss', [0.6418470144271851, 0.5593952238559723, 0.5430145025253296]), ('accuracy', [0.6159489329268293, 0.680201981707317, 0.707398056402439])]), 'n_epochs': 3, 'stopped_epoch': 2, 'monitor': 'accuracy', 'best_epoch': 2, 'best_score': 0.707398056402439, 'best_chk': '../output/testexp/train.0.model_2.chk'}, writing to ../output/testexp/train.0.result.json\n",
      "2023-11-22 17:04:16 [INFO]: End of 0 train round.\n",
      "2023-11-22 17:04:16 [INFO]: Setting model states with the best checkpoint ../output/testexp/train.0.model_2.chk\n",
      "2023-11-22 17:04:17 [INFO]: Loaded best model states from ../output/testexp/train.0.model_2.chk\n",
      "2023-11-22 17:04:17 [INFO]: Start 1 train round using data: iedb_sars2, round_conf: {'data': 'iedb_sars2', 'test_size': 0.2, 'batch_size': 256, 'n_epochs': 3, 'n_workers': 12, 'metrics': ['accuracy'], 'optimizer': {'type': 'adamw', 'lr': 0.0001}, 'train_bert_encoders': [-6, None], 'early_stopper': {'monitor': 'accuracy', 'patience': 1}, 'model_checkpoint': {'chk': 'train.{round}.model_{epoch}.chk', 'monitor': 'accuracy', 'save_best_only': True, 'period': 1}, 'result': 'train.{round}.result.json'}\n",
      "2023-11-22 17:04:19 [INFO]: iedb_sars2 dataset was loaded from ../output/iedb_sars2.data.csv, df.shape: (98563, 10)\n",
      "2023-11-22 17:04:19 [INFO]: Start to exclude eval data from train data, df.shape: (98563, 10), target_cols: ['index']\n",
      "2023-11-22 17:04:19 [INFO]: shomuradova dataset was loaded from ../output/shomuradova.data.csv, df.shape: (610, 9)\n",
      "2023-11-22 17:04:19 [INFO]: Excluding shomuradova eval data by index from train data\n",
      "2023-11-22 17:04:19 [INFO]: Current train data.shape: (98563, 10)\n",
      "2023-11-22 17:04:19 [INFO]: immunecode dataset was loaded from ../output/immunecode.data.csv, df.shape: (742, 9)\n",
      "2023-11-22 17:04:19 [INFO]: Excluding immunecode eval data by index from train data\n",
      "2023-11-22 17:04:20 [INFO]: Current train data.shape: (98563, 10)\n",
      "2023-11-22 17:04:20 [INFO]: Final train data.shape: (98563, 10)\n",
      "2023-11-22 17:04:20 [INFO]: The bert encoders to be trained: [-6, None]\n",
      "2023-11-22 17:04:20 [INFO]: ======================\n",
      "2023-11-22 17:04:20 [INFO]: Begin training...\n",
      "2023-11-22 17:04:20 [INFO]: use_cuda, device: True, cuda:0\n",
      "2023-11-22 17:04:20 [INFO]: train.n_data: 78850, test.n_data: 19713\n",
      "2023-11-22 17:04:20 [INFO]: optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "2023-11-22 17:04:20 [INFO]: evaluator: <tcrbert.model.BertTCREpitopeModel.PredictionEvaluator object at 0x7ff3351f4c40>\n",
      "2023-11-22 17:04:20 [INFO]: n_epochs: 3\n",
      "2023-11-22 17:04:20 [INFO]: train.batch_size: 256\n",
      "2023-11-22 17:04:20 [INFO]: test.batch_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in epoch 0/3: 100%|██████████| 309/309 [03:35<00:00,  1.44batch/s]\n",
      "Validating in epoch 0/3:  99%|█████████▊| 77/78 [00:42<00:00,  1.80batch/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/hym/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/hym/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\nTypeError: forward() missing 1 required positional argument: 'input_ids'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m metrics \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroc_auc\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[43mexperiment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Backup the train results\u001B[39;00m\n\u001B[1;32m     21\u001B[0m experiment\u001B[38;5;241m.\u001B[39mbackup_train_results()\n",
      "File \u001B[0;32m~/trunk/TCRBert/tcrbert/exp.py:112\u001B[0m, in \u001B[0;36mExperiment.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    104\u001B[0m mc \u001B[38;5;241m=\u001B[39m ModelCheckpoint(score_recoder\u001B[38;5;241m=\u001B[39mscore_recoder,\n\u001B[1;32m    105\u001B[0m                      fn_chk\u001B[38;5;241m=\u001B[39mfn_chk,\n\u001B[1;32m    106\u001B[0m                      monitor\u001B[38;5;241m=\u001B[39mmonitor,\n\u001B[1;32m    107\u001B[0m                      save_best_only\u001B[38;5;241m=\u001B[39msave_best_only,\n\u001B[1;32m    108\u001B[0m                      period\u001B[38;5;241m=\u001B[39mperiod)\n\u001B[1;32m    109\u001B[0m model\u001B[38;5;241m.\u001B[39madd_train_listener(mc)\n\u001B[0;32m--> 112\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m          \u001B[49m\u001B[43mtest_data_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_data_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m          \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m          \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m          \u001B[49m\u001B[43muse_cuda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cuda\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m rd_result \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    120\u001B[0m rd_result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetrics\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m metrics\n",
      "File \u001B[0;32m~/trunk/TCRBert/tcrbert/model.py:209\u001B[0m, in \u001B[0;36mBertTCREpitopeModel.fit\u001B[0;34m(self, train_data_loader, test_data_loader, optimizer, metrics, n_epochs, use_cuda)\u001B[0m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;66;03m# Validation phase\u001B[39;00m\n\u001B[1;32m    208\u001B[0m         params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mphase\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 209\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fire_epoch_end(params)\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fire_train_end(params)\n",
      "File \u001B[0;32m~/trunk/TCRBert/tcrbert/model.py:382\u001B[0m, in \u001B[0;36mBertTCREpitopeModel._train_epoch\u001B[0;34m(self, data_loader, params)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 382\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m         loss \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mloss(outputs, targets)\n\u001B[1;32m    385\u001B[0m params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/trunk/TCRBert/tcrbert/model.py:296\u001B[0m, in \u001B[0;36mBertTCREpitopeModel.forward\u001B[0;34m(self, input_ids, input_mask)\u001B[0m\n\u001B[1;32m    293\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[BertTCREpitopeModel.forward]: input_ids: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (input_ids,\n\u001B[1;32m    294\u001B[0m                                                                   \u001B[38;5;28mstr\u001B[39m(input_ids\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;28;01mif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNone\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# bert_out: # sequence_output, pooled_output, (hidden_states), (attentions)\u001B[39;00m\n\u001B[0;32m--> 296\u001B[0m bert_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;66;03m# sequence_out.shape: (batch_size, seq_len, hidden_size), pooled_out.shape: (batch_size, hidden_size)\u001B[39;00m\n\u001B[1;32m    298\u001B[0m sequence_out, pooled_out \u001B[38;5;241m=\u001B[39m bert_out[:\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001B[0m, in \u001B[0;36mDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39minputs[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    170\u001B[0m replicas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplicate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids[:\u001B[38;5;28mlen\u001B[39m(inputs)])\n\u001B[0;32m--> 171\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparallel_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather(outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:181\u001B[0m, in \u001B[0;36mDataParallel.parallel_apply\u001B[0;34m(self, replicas, inputs, kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparallel_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, replicas, inputs, kwargs):\n\u001B[0;32m--> 181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparallel_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mreplicas\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:89\u001B[0m, in \u001B[0;36mparallel_apply\u001B[0;34m(modules, inputs, kwargs_tup, devices)\u001B[0m\n\u001B[1;32m     87\u001B[0m     output \u001B[38;5;241m=\u001B[39m results[i]\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ExceptionWrapper):\n\u001B[0;32m---> 89\u001B[0m         \u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     outputs\u001B[38;5;241m.\u001B[39mappend(output)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/_utils.py:644\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 644\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mTypeError\u001B[0m: Caught TypeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/hym/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/hym/miniconda3/envs/TCRBert/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\nTypeError: forward() missing 1 required positional argument: 'input_ids'\n"
     ]
    }
   ],
   "source": [
    "from tcrbert.dataset import TCREpitopeSentenceDataset, CN\n",
    "from collections import OrderedDict, Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sh_ds = TCREpitopeSentenceDataset.from_key('shomuradova')\n",
    "sh_df = sh_ds.df_enc\n",
    "im_ds = TCREpitopeSentenceDataset.from_key('immunecode')\n",
    "\n",
    "# Remove duplicated CDR3beta seqs with Shomuradova\n",
    "im_ds.df_enc = im_ds.df_enc[\n",
    "        im_ds.df_enc[CN.cdr3b].map(lambda seq: seq not in sh_df[CN.cdr3b].values)\n",
    "]\n",
    "im_df = im_ds.df_enc\n",
    "\n",
    "metrics = ['accuracy', 'f1', 'roc_auc']\n",
    "\n",
    "# Train\n",
    "experiment.train()\n",
    "\n",
    "# Backup the train results\n",
    "experiment.backup_train_results()\n",
    "\n",
    "# Get best model and evaluate the model\n",
    "model = experiment.load_eval_model()\n",
    "eval_recoder = PredResultRecoder(output_attentions=True, output_hidden_states=True)\n",
    "model.add_pred_listener(eval_recoder)    \n",
    "data_loader = DataLoader(ds, batch_size=len(ds), shuffle=False, num_workers=2)\n",
    "logger.info('Predicting for %s' % ds.name)\n",
    "model.predict(data_loader=data_loader, metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "639px",
    "left": "1740px",
    "right": "20px",
    "top": "120px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
